{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GuSujpSX4hZg"
   },
   "source": [
    "# (KOR) Train a Vision Transformer on small datasets\n",
    "\n",
    "**저자:** [Aritra Roy Gosthipaty](https://twitter.com/ariG23498)<br>\n",
    "**만든 날:** 2022/01/07<br>\n",
    "**최종수정:** 2022/01/10<br>\n",
    "**간단설명:** shifted patch tokenizaiton과 locality self-attention을 적용하여 작은 크기의 데이터셋을 활용한 ViT학습을 밑바닥부터 수행해 봅니다.<br>\n",
    "**한글번역:** [모두의연구소 박은수](https://www.linkedin.com/in/eunsoo/) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vPfoaMmz4hZj"
   },
   "source": [
    "## Introduction\n",
    "Vision Transformer(ViT)를 설명하는 논문인 [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)에서 ViT는 엄청난 수의 데이터를 필요로 한다고 합니다. 따라서 ViT를 JFM300M과 같은 큰 사이즈의 데이터로 사전학습(pretraining)하고 이를 ImageNet과 같은 중간크기 사이즈의 데이터로 미세조정(fine-tuning)하는 방법이 State-of-the-art 성능의 Convolutional Neural Networks(CNNs)모델의 성능을 넘어서는 유일한 방법이라고 했습니다.\n",
    "\n",
    "ViT의 self-attention레이어는 이미지가 **locality inductive bias** (이미지 픽셀들이 가까운곳끼리(locally) 관련되어있으며, 그것의 상관맵(correlation maps)이 이동불변)하다는 특징을 활용하지 않습니다. 바로 이점이 ViT가 더 많은 데이터를 필요로하는 이유입니다. 반면에 CNNs은 슬라이딩 윈도우 형태로 이미지를 처리하기 때문에 더 적은 수의 데이터를 갖고도 좋은 성능을 얻을 수 있습니다.\n",
    "\n",
    "[Vision Transformer for Small-Size Datasets](https://arxiv.org/abs/2112.13492v1)논문의 저자들은 ViT가 갖고있는 locality inductive bias문제를 해결하고자 합니다. \n",
    "\n",
    "핵심 아이디어는 다음과 같습니다 : \n",
    "- **Shifted Patch Tokenization**\n",
    "- **Locality Self Attention** \n",
    "\n",
    "이 예제는 이 논문의 아이디어를 구현한 것입니다. 이 코드의 많은 부분을 [Image classification with Vision Transformer](https://keras.io/examples/vision/image_classification_with_vision_transformer/)에서 영감을 받아 작성하였습니다.\n",
    "\n",
    "_참고_ : 이 예제는 TensorFlow 2.6 이상의 버젼과 [TensorFlow Addons](https://www.tensorflow.org/addons)이 필요합니다. TensorFlow Addons의 경우 다음 명령어로 설치할 수 있습니다. \n",
    "\n",
    "```python\n",
    "pip install -qq -U tensorflow-addons\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nzbb4z-r4hZj"
   },
   "source": [
    "## 셋업 (Setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rL8VgbnH4hZk"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# 재현할 수 있도록 시드 값 설정\n",
    "SEED = 42\n",
    "keras.utils.set_random_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RissI0bH4hZl"
   },
   "source": [
    "## 데이터 준비 (Prepare the data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L9SxN4484hZl"
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = 100\n",
    "INPUT_SHAPE = (32, 32, 3)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n",
    "\n",
    "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ulJMoy-Y4hZm"
   },
   "source": [
    "## 하이퍼 파라미터 구성 (Configure the hyperparameters)\n",
    "이 예제의 하이퍼파라미터는 논문과 다릅니다. 하이퍼파라미터로 자유롭게 바꿔보셔도 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "adDR19V24hZm"
   },
   "outputs": [],
   "source": [
    "# 데이터\n",
    "BUFFER_SIZE = 512\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# 데이터 증강 (AUGMENTATION)\n",
    "IMAGE_SIZE = 72\n",
    "PATCH_SIZE = 6\n",
    "NUM_PATCHES = (IMAGE_SIZE // PATCH_SIZE) ** 2\n",
    "\n",
    "# 옵티마이저 (OPTIMIZER)\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 0.0001\n",
    "\n",
    "# 헉습 (TRAINING)\n",
    "EPOCHS = 50\n",
    "\n",
    "# 구조 (ARCHITECTURE)\n",
    "LAYER_NORM_EPS = 1e-6\n",
    "TRANSFORMER_LAYERS = 8\n",
    "PROJECTION_DIM = 64\n",
    "NUM_HEADS = 4\n",
    "TRANSFORMER_UNITS = [\n",
    "    PROJECTION_DIM * 2,\n",
    "    PROJECTION_DIM,\n",
    "]\n",
    "MLP_HEAD_UNITS = [2048, 1024]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u_tzyxKO4hZn"
   },
   "source": [
    "## 데이터 증강 (Use data augmentation)\n",
    "\n",
    "논문에서 언급한 부분 : \n",
    "*\"DeiT논문에서 ViT를 효과적으로 학습하기위해서는 다양한 방법이 필요하다고한다. 따라서 CutMix, Mixup, Auto Augment, Repeated Augment 같은 데이터 증강방법을 모든 모델에 적용하였다.\"*\n",
    "\n",
    "이 예제에서는 논문을 그대로 재현하는것이 아니라 논문에서 제시한 참신성(novelty)에만 초점을 둘것 입니다. 따라서 위 논문에서 언급되었던 데이터증강을 사용하지 않을 것입니다. 그렇기에 자유롭게 데이터증강을 위한 파이프라인을 추가하거나 삭제하셔도 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5C9EURZL4hZn"
   },
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.Normalization(),\n",
    "        layers.Resizing(IMAGE_SIZE, IMAGE_SIZE),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(factor=0.02),\n",
    "        layers.RandomZoom(height_factor=0.2, width_factor=0.2),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")\n",
    "# 노말라이제이션(normalizaiton)을 위해서 학습데이터의 mean과 variance를 계산\n",
    "data_augmentation.layers[0].adapt(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KZQyJqp54hZo"
   },
   "source": [
    "## Patch Tokenization의 구현 (Implement Shifted Patch Tokenization)\n",
    "ViT의 파이프라인에서 입력 이미지는 선형투영(linear projection)을 통해 토큰으로 바뀝니다. ViT는 작은 크기의 recpetive field를 갖고 있기 때문에 이를 극복하고자 Shifted Patch Tokenizaiton (STP)가 도입되었습니다. Shifted Patch Tokenization은 다음 단계를 따릅니다. \n",
    "\n",
    "- 이미지로 시작\n",
    "- 이미지를 대각선 방향으로 이동\n",
    "- 대각선으로 이동한 이미지들과 원본 이미지를 채널 방향으로 연결(Concat)\n",
    "- 연결된 이미지에서 패치들을 추출\n",
    "- 모든 패치의 공간차원(spatial dimension)을 벡터형태로 변환 (flatten)\n",
    "- 벡터형태의 패치에 레이어 노말라이제이션을 적용한 후 다시 선형투영 (linear projection)\n",
    "\n",
    "| ![Shifted Patch Toekenization](https://i.imgur.com/bUnHxd0.png) |\n",
    "| :--: |\n",
    "| Shifted Patch Tokenization [Source](https://arxiv.org/abs/2112.13492v1) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mnKTHWbS4hZo"
   },
   "outputs": [],
   "source": [
    "# from IPython.core.debugger import set_trace\n",
    "class ShiftedPatchTokenization(layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size=IMAGE_SIZE,\n",
    "        patch_size=PATCH_SIZE,\n",
    "        num_patches=NUM_PATCHES,\n",
    "        projection_dim=PROJECTION_DIM,\n",
    "        vanilla=False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.vanilla = vanilla  # 바닐라 패치 추출기로 전환할 수 있는 플래그\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.half_patch = patch_size // 2\n",
    "        self.flatten_patches = layers.Reshape((num_patches, -1))\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.layer_norm = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)\n",
    "\n",
    "    def crop_shift_pad(self, images, mode):\n",
    "        # 대각방향으로 이동된 이미지 만들기\n",
    "        if mode == \"left-up\":\n",
    "            crop_height = self.half_patch\n",
    "            crop_width = self.half_patch\n",
    "            shift_height = 0\n",
    "            shift_width = 0\n",
    "        elif mode == \"left-down\":\n",
    "            crop_height = 0\n",
    "            crop_width = self.half_patch\n",
    "            shift_height = self.half_patch\n",
    "            shift_width = 0\n",
    "        elif mode == \"right-up\":\n",
    "            crop_height = self.half_patch\n",
    "            crop_width = 0\n",
    "            shift_height = 0\n",
    "            shift_width = self.half_patch\n",
    "        else:\n",
    "            crop_height = 0\n",
    "            crop_width = 0\n",
    "            shift_height = self.half_patch\n",
    "            shift_width = self.half_patch\n",
    "\n",
    "        # 대각이동 이미지 만들고 자른 후 패딩하기\n",
    "        crop = tf.image.crop_to_bounding_box(\n",
    "            images,\n",
    "            offset_height=crop_height,\n",
    "            offset_width=crop_width,\n",
    "            target_height=self.image_size - self.half_patch,\n",
    "            target_width=self.image_size - self.half_patch,\n",
    "        )\n",
    "        shift_pad = tf.image.pad_to_bounding_box(\n",
    "            crop,\n",
    "            offset_height=shift_height,\n",
    "            offset_width=shift_width,\n",
    "            target_height=self.image_size,\n",
    "            target_width=self.image_size,\n",
    "        )\n",
    "        return shift_pad\n",
    "\n",
    "    def call(self, images):\n",
    "        if not self.vanilla:\n",
    "            # 원본 이미지와 대각이동 이미지들의 concat\n",
    "            images = tf.concat(\n",
    "                [\n",
    "                    images,\n",
    "                    self.crop_shift_pad(images, mode=\"left-up\"),\n",
    "                    self.crop_shift_pad(images, mode=\"left-down\"),\n",
    "                    self.crop_shift_pad(images, mode=\"right-up\"),\n",
    "                    self.crop_shift_pad(images, mode=\"right-down\"),\n",
    "                ],\n",
    "                axis=-1,\n",
    "            )\n",
    "        # 이미지를 패치로 만들고 패치들을 벡터형태로 변환하기 (flatten)\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        flat_patches = self.flatten_patches(patches)\n",
    "        if not self.vanilla:\n",
    "            # 레이어 노말라이제이션 적용 후 선형투영\n",
    "            tokens = self.layer_norm(flat_patches)\n",
    "            tokens = self.projection(tokens)\n",
    "        else:\n",
    "            # 선형투영 \n",
    "            tokens = self.projection(flat_patches)\n",
    "        return (tokens, patches)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0DRe7qrY4hZo"
   },
   "source": [
    "### 패치를 시각화 하기 (Visualize the patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "53AbIyK74hZp"
   },
   "outputs": [],
   "source": [
    "# 학습데이터셋에서 랜덤 이미지를 갖고 온 후 \n",
    "# 이미지 리사이즈 적용\n",
    "image = x_train[np.random.choice(range(x_train.shape[0]))]\n",
    "resized_image = tf.image.resize(\n",
    "    tf.convert_to_tensor([image]), size=(IMAGE_SIZE, IMAGE_SIZE)\n",
    ")\n",
    "# 바닐라 패치메이커 : ViT 논문에서 처럼 이미지를 받아서 패치로 변환함. \n",
    "(token, patch) = ShiftedPatchTokenization(vanilla=True)(resized_image / 255.0)\n",
    "(token, patch) = (token[0], patch[0])\n",
    "n = patch.shape[0]\n",
    "\n",
    "# 번역자 주석추가 : 바닐라 패치메이커로 한장 추출 후 패치로 만들어 보기\n",
    "count = 1\n",
    "plt.figure(figsize=(4, 4))\n",
    "for row in range(n):\n",
    "    for col in range(n):\n",
    "        plt.subplot(n, n, count)\n",
    "        count = count + 1\n",
    "        image = tf.reshape(patch[row][col], (PATCH_SIZE, PATCH_SIZE, 3))\n",
    "        plt.imshow(image)\n",
    "        plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "53AbIyK74hZp"
   },
   "outputs": [],
   "source": [
    "# Shifted Patch Tokenization : 입력 이미지에 4개의 대각선 이동을 추가한 후 \n",
    "# 이를 채널 방향으로 연결(concat) 한 뒤 패치를 추출\n",
    "(token, patch) = ShiftedPatchTokenization(vanilla=False)(resized_image / 255.0)\n",
    "(token, patch) = (token[0], patch[0])\n",
    "n = patch.shape[0]\n",
    "shifted_images = [\"ORIGINAL\", \"LEFT-UP\", \"LEFT-DOWN\", \"RIGHT-UP\", \"RIGHT-DOWN\"]\n",
    "for index, name in enumerate(shifted_images):\n",
    "    print(name)\n",
    "    count = 1\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    for row in range(n):\n",
    "        for col in range(n):\n",
    "            plt.subplot(n, n, count)\n",
    "            count = count + 1\n",
    "            image = tf.reshape(patch[row][col], (PATCH_SIZE, PATCH_SIZE, 5 * 3))\n",
    "            plt.imshow(image[..., 3 * index : 3 * index + 3])\n",
    "            plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bCEdAPot4hZp"
   },
   "source": [
    "## Patch Encoding 레이어의 구현 (Implement the patch encoding layer)\n",
    "프로젝션 된 패치를 입력으로 받은 후 위치정보(positional information)를 추가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kwzhDH3w4hZp"
   },
   "outputs": [],
   "source": [
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(\n",
    "        self, num_patches=NUM_PATCHES, projection_dim=PROJECTION_DIM, **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_patches = num_patches\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "        self.positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "\n",
    "    def call(self, encoded_patches):\n",
    "        encoded_positions = self.position_embedding(self.positions)\n",
    "        encoded_patches = encoded_patches + encoded_positions\n",
    "        return encoded_patches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7BUwOyP34hZq"
   },
   "source": [
    "## Locality Self Attention의 구현 (Implement Locality Self Attention)\n",
    "\n",
    "어텐션(attention)의 일반수식은 아래와 같습니다. \n",
    "\n",
    "| ![Equation of attention](https://miro.medium.com/max/396/1*P9sV1xXM10t943bXy_G9yg.png) |\n",
    "| :--: |\n",
    "| [Source](https://towardsdatascience.com/attention-is-all-you-need-discovering-the-transformer-paper-73e5ff5e0634) |\n",
    "\n",
    "어텐션 모듈은 쿼리(query), 키(key), 벨류(value) 값을 사용합니다. 먼저 내적(dot prodcut)를 이용하여 쿼리와 키의 유사도를 계산합니다. 그 값은 키(key) 차원의 제곱근(squre root)으로 그 값이 조정됩니다. 이 조정을 통해 softmax함수가 굉장히 작은 기울기 값을 갖는 것을 방지할 수 있습니다. 이제 이 값은 Softmax 계산을 통해 어텐션을 위한 가중치 값으로 변하게 되고 이 어텐션 가중치가 벨류 값에 곱해져서 값의 조정이 발생합니다.\n",
    "\n",
    "셀프 어텐션에서는 쿼리, 키, 벨류 값이 전부 동일한 입력을 갖습니다. \n",
    "이 내적의 계산을 통해서 토큰 간의 관계값(inter-token relations)보다는 큰 값의 셀프토큰 관계값(self-token relations)를 얻을 수 있습니다.\n",
    "이것은 softmax가 토큰 간의 관계값 보다 셀프토큰 관계값에 더 큰 확률을 부여한다는 것을 의미합니다. \n",
    "이를 방지하기 위하여 저자들은 **내적의 대각선 값들을 마스킹**하는 것을 제안합니다. \n",
    "이 방법으로 어텐션 모듈이 토큰 간의 관계값 계산에 더 집중하도록 할 수 있습니다. \n",
    "\n",
    "일반적인 어텐션 모듈에서 스케일링 팩터(sacling factor)는 상수(constant) 입니다. \n",
    "이것은 sofmax 함수를 조절할 수 있는 온도 항처럼 사용 됩니다. \n",
    "저자들은 **온도 항을 상수항이 아닌 학습 가능한 형태로 사용**하는 것을 제안합니다. \n",
    "\n",
    "\n",
    "| ![Implementation of LSA](https://i.imgur.com/GTV99pk.png) |\n",
    "| :--: |\n",
    "| Locality Self Attention [Source](https://arxiv.org/abs/2112.13492v1) |\n",
    "\n",
    "위의 두 아이디어가 바로 Locality Self Attention을 만듭니다. 예제에서는 이를 위해서 [`layers.MultiHeadAttention`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MultiHeadAttention)를 서브클래싱하고 학습 가능한 온도 항을 만들었습니다. 어텐션 마스크(attnetion mask)는 다음 스테이지에서 구현됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oZQ-6wF84hZq"
   },
   "outputs": [],
   "source": [
    "\n",
    "class MultiHeadAttentionLSA(tf.keras.layers.MultiHeadAttention):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # 학습 가능한 온도 텀. 초기 값은 키(key) 차원의 제곱근 값 (squre root)\n",
    "        self.tau = tf.Variable(math.sqrt(float(self._key_dim)), trainable=True)\n",
    "\n",
    "    def _compute_attention(self, query, key, value, attention_mask=None, training=None):\n",
    "        query = tf.multiply(query, 1.0 / self.tau)\n",
    "        attention_scores = tf.einsum(self._dot_product_equation, key, query)\n",
    "        attention_scores = self._masked_softmax(attention_scores, attention_mask)\n",
    "        attention_scores_dropout = self._dropout_layer(\n",
    "            attention_scores, training=training\n",
    "        )\n",
    "        attention_output = tf.einsum(\n",
    "            self._combine_equation, attention_scores_dropout, value\n",
    "        )\n",
    "        return attention_output, attention_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j1K45O9m4hZq"
   },
   "source": [
    "## MLP 구현하기 (Implement the MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1rvnSLVb4hZq"
   },
   "outputs": [],
   "source": [
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "# Build the diagonal attention mask\n",
    "diag_attn_mask = 1 - tf.eye(NUM_PATCHES)\n",
    "diag_attn_mask = tf.cast([diag_attn_mask], dtype=tf.int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tS9x-yHq4hZr"
   },
   "source": [
    "## ViT 만들기 (Build the ViT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ldINSs4V4hZr"
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_vit_classifier(vanilla=False):\n",
    "    inputs = layers.Input(shape=INPUT_SHAPE)\n",
    "    # 데이터 증강\n",
    "    augmented = data_augmentation(inputs)\n",
    "    # 패치 생성\n",
    "    (tokens, _) = ShiftedPatchTokenization(vanilla=vanilla)(augmented)\n",
    "    # 패치 인코딩\n",
    "    encoded_patches = PatchEncoder()(tokens)\n",
    "\n",
    "    # 다중 레이어의 트렌스포머 블록 만들기\n",
    "    for _ in range(TRANSFORMER_LAYERS):\n",
    "        # 레이어 노말라이제이션 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        # 멀티-헤드 어텐션 레이어 만들기.\n",
    "        if not vanilla:\n",
    "            attention_output = MultiHeadAttentionLSA(\n",
    "                num_heads=NUM_HEADS, key_dim=PROJECTION_DIM, dropout=0.1\n",
    "            )(x1, x1, attention_mask=diag_attn_mask)\n",
    "        else:\n",
    "            attention_output = layers.MultiHeadAttention(\n",
    "                num_heads=NUM_HEADS, key_dim=PROJECTION_DIM, dropout=0.1\n",
    "            )(x1, x1)\n",
    "        # 스캡 커넥션 1.\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "        # 레이어 노말라이제이션 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=TRANSFORMER_UNITS, dropout_rate=0.1)\n",
    "        # 스킵 커넥션 2.\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # [batch_size, projection_dim] 크기의 텐서(tensor) 만들기.\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = layers.Flatten()(representation)\n",
    "    representation = layers.Dropout(0.5)(representation)\n",
    "    # MLP 추가\n",
    "    features = mlp(representation, hidden_units=MLP_HEAD_UNITS, dropout_rate=0.5)\n",
    "    # 출력을 분류하기\n",
    "    logits = layers.Dense(NUM_CLASSES)(features)\n",
    "    # Keras 모델로 만들기 \n",
    "    model = keras.Model(inputs=inputs, outputs=logits)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CbrcvPiQ4hZr"
   },
   "source": [
    "## 컴파일, 학습, 모드 평가 (Compile, train, and evaluate the mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YK3zYkOU4hZr"
   },
   "outputs": [],
   "source": [
    "# 몇몇 코드는 아래의 링크에서 가져 왔습니다 : \n",
    "# https://www.kaggle.com/ashusma/training-rfcx-tensorflow-tpu-effnet-b2.\n",
    "class WarmUpCosine(keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(\n",
    "        self, learning_rate_base, total_steps, warmup_learning_rate, warmup_steps\n",
    "    ):\n",
    "        super(WarmUpCosine, self).__init__()\n",
    "\n",
    "        self.learning_rate_base = learning_rate_base\n",
    "        self.total_steps = total_steps\n",
    "        self.warmup_learning_rate = warmup_learning_rate\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.pi = tf.constant(np.pi)\n",
    "\n",
    "    def __call__(self, step):\n",
    "        if self.total_steps < self.warmup_steps:\n",
    "            raise ValueError(\"Total_steps must be larger or equal to warmup_steps.\")\n",
    "\n",
    "        cos_annealed_lr = tf.cos(\n",
    "            self.pi\n",
    "            * (tf.cast(step, tf.float32) - self.warmup_steps)\n",
    "            / float(self.total_steps - self.warmup_steps)\n",
    "        )\n",
    "        learning_rate = 0.5 * self.learning_rate_base * (1 + cos_annealed_lr)\n",
    "\n",
    "        if self.warmup_steps > 0:\n",
    "            if self.learning_rate_base < self.warmup_learning_rate:\n",
    "                raise ValueError(\n",
    "                    \"Learning_rate_base must be larger or equal to \"\n",
    "                    \"warmup_learning_rate.\"\n",
    "                )\n",
    "            slope = (\n",
    "                self.learning_rate_base - self.warmup_learning_rate\n",
    "            ) / self.warmup_steps\n",
    "            warmup_rate = slope * tf.cast(step, tf.float32) + self.warmup_learning_rate\n",
    "            learning_rate = tf.where(\n",
    "                step < self.warmup_steps, warmup_rate, learning_rate\n",
    "            )\n",
    "        return tf.where(\n",
    "            step > self.total_steps, 0.0, learning_rate, name=\"learning_rate\"\n",
    "        )\n",
    "\n",
    "\n",
    "def run_experiment(model):\n",
    "    total_steps = int((len(x_train) / BATCH_SIZE) * EPOCHS)\n",
    "    warmup_epoch_percentage = 0.10\n",
    "    warmup_steps = int(total_steps * warmup_epoch_percentage)\n",
    "    scheduled_lrs = WarmUpCosine(\n",
    "        learning_rate_base=LEARNING_RATE,\n",
    "        total_steps=total_steps,\n",
    "        warmup_learning_rate=0.0,\n",
    "        warmup_steps=warmup_steps,\n",
    "    )\n",
    "\n",
    "    optimizer = tfa.optimizers.AdamW(\n",
    "        learning_rate=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\n",
    "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        validation_split=0.1,\n",
    "    )\n",
    "    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test, batch_size=BATCH_SIZE)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "# 바닐라 ViT로 실험.\n",
    "vit = create_vit_classifier(vanilla=True)\n",
    "history = run_experiment(vit)\n",
    "\n",
    "# Shifted Patch Tokenizaiton과 \n",
    "# Locality Self Attention을 적용한 ViT 실험\n",
    "vit_sl = create_vit_classifier(vanilla=False)\n",
    "history = run_experiment(vit_sl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0KBuvt9Z4hZs"
   },
   "source": [
    "# 마무리 하며\n",
    "Shifted Patch Tokenization과 Locality Self Attention을 이용하여 CIFAR100에서 ~**3-4%** top-1 정확도를 얻을 수 있었습니다. \n",
    "\n",
    "Shifted Patch Tokenization과 Locality Self Attention에 대한 아이디어는 매우 직관적이고 구현하기 쉽습니다. 저자는 논문의 부록에서 Shifted Patch Tokenization에 대한 서로 다른 shit전략에 대한 실험결과도 제시하였습니다. \n",
    "\n",
    "GPU 크레딧으로 실험할 수 있는 환경을 제공한 [Jarvislabs.ai](https://jarvislabs.ai/)에 감사 드립니다.\n",
    "\n",
    "학습 완료된 모델을 [Hugging Face Hub](https://huggingface.co/keras-io/vit_small_ds_v2)에서 사용해 볼 수 있으며 데모 또한 [Hugging Face Spaces](https://huggingface.co/spaces/keras-io/vit-small-ds)에서 확인하실 수 있습니다."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "vit_small_ds",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

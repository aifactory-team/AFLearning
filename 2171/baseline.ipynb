{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d1oF4zhwMve"
      },
      "source": [
        "# Distilling Vision Transformers\n",
        "\n",
        "**Author:** [Sayak Paul](https://twitter.com/RisingSayak)<br>\n",
        "**Date created:** 2022/04/05<br>\n",
        "**Last modified:** 2022/04/08<br>\n",
        "**Description:** Distillation of Vision Transformers through attention.<br>\n",
        "**Translation:** [Junghyun Park](https://github.com/parkjh688)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4YWyD3VwMvg"
      },
      "source": [
        "## 들어가며\n",
        "\n",
        "*Vision Transformers* (ViT) 논문 ([Dosovitskiy et al.](https://arxiv.org/abs/2010.11929))에서 저자들은 ViTs가 Convolutional Neural Networks(CNNs)에 비하는 성능을 내려면 더 큰 데이터셋으로 pre-train을 진행해야한다고 했습니다. 그리고 그 데이터셋은 크면 클수록 좋습니다. ViT 아키텍쳐엔 CNNs과는 달리 inductive biases와 지역성(locality)에 대한 레이어가 없기 때문입니다. 이후의 논문 [Steiner et al.](https://arxiv.org/abs/2106.10270)에서 저자들은 더 강력한 정규화(regularization)와 더 긴 학습으로 ViT의 성능을 실질적으로 향상시키는 것이 가능하다는 것을 보여줍니다.\n",
        "\n",
        "많은 그룹이 ViT 학습시에 data-intensiveness 문제를 다루는 다양한 방법을 제안했습니다. 그 방법 중 하나는 *Data-efficient image Transformers*, (DeiT) [Touvron et al.](https://arxiv.org/abs/2012.12877)에서 제안했으며, 저자들은 vision transformer 모델에 특화된 distillation 방식을 소개했습니다. DeiT는 더 큰 데이터 셋을 사용하지 않고도 ViT를 잘 학습할 수 있다는 것을 보여줬습니다.\n",
        "\n",
        "이 튜토리얼에서 우리는 DeiT 논문에서 제안한 distillation 레시피를 구현할 것이고, 구현을 위해 ViT 아키텍처를 약간 수정하고 distillation 레시피를 구현하기 위한 custom training loop를 만들 것입니다.\n",
        "\n",
        "튜토리얼을 실행하려면 다음 명령을 사용하여 설치할 수 있는 TensorFlow Addons을 설치해야합니다.\n",
        "```\n",
        "pip install tensorflow-addons\n",
        "```\n",
        "\n",
        "이 튜토리얼을 쉽게 이해하려면 일단 ViT와 knowledge distillation이 어떻게 동작하는지 아는 것이 좋습니다. 아래의 리소스가 당신을 도와줄 것입니다:\n",
        "\n",
        "* [ViT on keras.io](https://keras.io/examples/vision/image_classification_with_vision_transformer)\n",
        "* [Knowledge distillation on keras.io](https://keras.io/examples/vision/knowledge_distillation/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_JdZAkzwMvh"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# colab에는 tensorflow-addons 설치 되어있지 않으므로 설치\n",
        "!pip install tensorflow-addons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KdTswX36x1Zn",
        "outputId": "43c01f4e-47a6-4ff5-cde2-ad4b8d28d51f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.18.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow-addons) (3.0.9)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJ6f4AFOwMvh"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "tfds.disable_progress_bar()\n",
        "tf.keras.utils.set_random_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4_UPYafwMvh"
      },
      "source": [
        "## Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSEJia5lwMvi"
      },
      "outputs": [],
      "source": [
        "# Model\n",
        "MODEL_TYPE = \"deit_distilled_tiny_patch16_224\"\n",
        "RESOLUTION = 224\n",
        "PATCH_SIZE = 16\n",
        "NUM_PATCHES = (RESOLUTION // PATCH_SIZE) ** 2\n",
        "LAYER_NORM_EPS = 1e-6\n",
        "PROJECTION_DIM = 192\n",
        "NUM_HEADS = 3\n",
        "NUM_LAYERS = 12\n",
        "MLP_UNITS = [\n",
        "    PROJECTION_DIM * 4,\n",
        "    PROJECTION_DIM,\n",
        "]\n",
        "DROPOUT_RATE = 0.0\n",
        "DROP_PATH_RATE = 0.1\n",
        "\n",
        "# Training\n",
        "NUM_EPOCHS = 20\n",
        "BASE_LR = 0.0005\n",
        "WEIGHT_DECAY = 0.0001\n",
        "\n",
        "# Data\n",
        "BATCH_SIZE = 256\n",
        "AUTO = tf.data.AUTOTUNE\n",
        "NUM_CLASSES = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZhc15xGwMvj"
      },
      "source": [
        "**DROPOUT_RATE**가 0.0으로 설정되었다는 것을 눈치채셨을 것 같습니다. 이 예제에 사용된 작은(smaller) 모델의 경우 필요하지 않지만 큰(bigger) 모델의 경우 드롭아웃을 사용합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgVQxEZywMvj"
      },
      "source": [
        "## `tf_flowers` 데이터셋 그리고 전처리 유틸리티 가져오기\n",
        "\n",
        "MixUp ([Zhang et al.](https://arxiv.org/abs/1710.09412))과 RandAugment ([Cubuk et al.](https://arxiv.org/abs/1909.13719))의 저자들은 다양한 augmentation 기법을 사용했습니다. </br>\n",
        "그러나 튜토리얼의 단순화를 위하여 우리는 그 부분은 사용하지 않을 것입니다"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s55b8MYJwMvj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28d144ef-d9b5-4ed0-cffc-b60c154143d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mDownloading and preparing dataset 218.21 MiB (download: 218.21 MiB, generated: 221.83 MiB, total: 440.05 MiB) to ~/tensorflow_datasets/tf_flowers/3.0.1...\u001b[0m\n",
            "\u001b[1mDataset tf_flowers downloaded and prepared to ~/tensorflow_datasets/tf_flowers/3.0.1. Subsequent calls will reuse this data.\u001b[0m\n",
            "Number of training examples: 3303\n",
            "Number of validation examples: 367\n"
          ]
        }
      ],
      "source": [
        "def preprocess_dataset(is_training=True):\n",
        "    def fn(image, label):\n",
        "        if is_training:\n",
        "            # 더 큰 공간 해상도로 크기를 리사이즈하고 랜덤 샘플링\n",
        "            # 크롭\n",
        "            image = tf.image.resize(image, (RESOLUTION + 20, RESOLUTION + 20))\n",
        "            image = tf.image.random_crop(image, (RESOLUTION, RESOLUTION, 3))\n",
        "            image = tf.image.random_flip_left_right(image)\n",
        "        else:\n",
        "            image = tf.image.resize(image, (RESOLUTION, RESOLUTION))\n",
        "        label = tf.one_hot(label, depth=NUM_CLASSES)\n",
        "        return image, label\n",
        "\n",
        "    return fn\n",
        "\n",
        "\n",
        "def prepare_dataset(dataset, is_training=True):\n",
        "    if is_training:\n",
        "        dataset = dataset.shuffle(BATCH_SIZE * 10)\n",
        "    dataset = dataset.map(preprocess_dataset(is_training), num_parallel_calls=AUTO)\n",
        "    return dataset.batch(BATCH_SIZE).prefetch(AUTO)\n",
        "\n",
        "\n",
        "train_dataset, val_dataset = tfds.load(\n",
        "    \"tf_flowers\", split=[\"train[:90%]\", \"train[90%:]\"], as_supervised=True\n",
        ")\n",
        "num_train = train_dataset.cardinality()\n",
        "num_val = val_dataset.cardinality()\n",
        "print(f\"Number of training examples: {num_train}\")\n",
        "print(f\"Number of validation examples: {num_val}\")\n",
        "\n",
        "train_dataset = prepare_dataset(train_dataset, is_training=True)\n",
        "val_dataset = prepare_dataset(val_dataset, is_training=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ri2sNkqqwMvk"
      },
      "source": [
        "### ViT 변형인 DeiT 구현하기\n",
        "\n",
        "DeiT는 ViT의 확장 버전이기 때문에 먼저 ViT를 구현한 다음 DeiT의 구성 요소를 추가하는 것이 알맞을 것입니다. </br>\n",
        "\n",
        "우선, 우리는 DeiT가 정규화(regularization)를 위해 사용하는 Stochastic Depth ([Huang et al.](https://arxiv.org/abs/1603.09382)) 레이어를 구현할겁니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEauIVWTwMvk"
      },
      "outputs": [],
      "source": [
        "# Referred from: github.com:rwightman/pytorch-image-models.\n",
        "class StochasticDepth(layers.Layer):\n",
        "    def __init__(self, drop_prop, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.drop_prob = drop_prop\n",
        "\n",
        "    def call(self, x, training=True):\n",
        "        if training:\n",
        "            keep_prob = 1 - self.drop_prob\n",
        "            shape = (tf.shape(x)[0],) + (1,) * (len(tf.shape(x)) - 1)\n",
        "            random_tensor = keep_prob + tf.random.uniform(shape, 0, 1)\n",
        "            random_tensor = tf.floor(random_tensor)\n",
        "            return (x / keep_prob) * random_tensor\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_UiEpL0wMvk"
      },
      "source": [
        "이제, MLP와 Transformer 블록을 구현해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SFLdAsoHwMvk"
      },
      "outputs": [],
      "source": [
        "\n",
        "def mlp(x, dropout_rate: float, hidden_units: List):\n",
        "    \"\"\"FFN for a Transformer block.\"\"\"\n",
        "    # hidden_units 만큼 반복하면서 Dense 레이어와 Dropout 레이어 추가\n",
        "    for (idx, units) in enumerate(hidden_units):\n",
        "        x = layers.Dense(\n",
        "            units,\n",
        "            activation=tf.nn.gelu if idx == 0 else None,\n",
        "        )(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def transformer(drop_prob: float, name: str) -> keras.Model:\n",
        "    \"\"\"Transformer block with pre-norm.\"\"\"\n",
        "    num_patches = NUM_PATCHES + 2 if \"distilled\" in MODEL_TYPE else NUM_PATCHES + 1\n",
        "    encoded_patches = layers.Input((num_patches, PROJECTION_DIM))\n",
        "\n",
        "    # Layer normalization 1.\n",
        "    x1 = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)(encoded_patches)\n",
        "\n",
        "    # Multi Head Self Attention layer 1.\n",
        "    attention_output = layers.MultiHeadAttention(\n",
        "        num_heads=NUM_HEADS,\n",
        "        key_dim=PROJECTION_DIM,\n",
        "        dropout=DROPOUT_RATE,\n",
        "    )(x1, x1)\n",
        "    attention_output = (\n",
        "        StochasticDepth(drop_prob)(attention_output) if drop_prob else attention_output\n",
        "    )\n",
        "\n",
        "    # Skip connection 1.\n",
        "    x2 = layers.Add()([attention_output, encoded_patches])\n",
        "\n",
        "    # Layer normalization 2.\n",
        "    x3 = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)(x2)\n",
        "\n",
        "    # MLP layer 1.\n",
        "    x4 = mlp(x3, hidden_units=MLP_UNITS, dropout_rate=DROPOUT_RATE)\n",
        "    x4 = StochasticDepth(drop_prob)(x4) if drop_prob else x4\n",
        "\n",
        "    # Skip connection 2.\n",
        "    outputs = layers.Add()([x2, x4])\n",
        "\n",
        "    return keras.Model(encoded_patches, outputs, name=name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QSEU9HrwMvl"
      },
      "source": [
        "\n",
        "이제 방금 개발한 요소 위에 `ViTClassifier` 클래스를 구현하겠습니다. 여기서는 ViT 논문에서 사용된 원래 풀링 방법을 그대로 따를 것입니다 - 클래스 토큰을 사용하고 분류에 해당하는 feature representation을 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PpTmuzLkwMvl"
      },
      "outputs": [],
      "source": [
        "\n",
        "class ViTClassifier(keras.Model):\n",
        "    \"\"\"Vision Transformer base class.\"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        # Patchify + linear projection + reshaping.\n",
        "        self.projection = keras.Sequential(\n",
        "            [\n",
        "                layers.Conv2D(\n",
        "                    filters=PROJECTION_DIM,\n",
        "                    kernel_size=(PATCH_SIZE, PATCH_SIZE),\n",
        "                    strides=(PATCH_SIZE, PATCH_SIZE),\n",
        "                    padding=\"VALID\",\n",
        "                    name=\"conv_projection\",\n",
        "                ),\n",
        "                layers.Reshape(\n",
        "                    target_shape=(NUM_PATCHES, PROJECTION_DIM),\n",
        "                    name=\"flatten_projection\",\n",
        "                ),\n",
        "            ],\n",
        "            name=\"projection\",\n",
        "        )\n",
        "\n",
        "        # Positional embedding.\n",
        "        init_shape = (\n",
        "            1,\n",
        "            NUM_PATCHES + 1,\n",
        "            PROJECTION_DIM,\n",
        "        )\n",
        "        self.positional_embedding = tf.Variable(\n",
        "            tf.zeros(init_shape), name=\"pos_embedding\"\n",
        "        )\n",
        "\n",
        "        # Transformer blocks.\n",
        "        dpr = [x for x in tf.linspace(0.0, DROP_PATH_RATE, NUM_LAYERS)]\n",
        "        self.transformer_blocks = [\n",
        "            transformer(drop_prob=dpr[i], name=f\"transformer_block_{i}\")\n",
        "            for i in range(NUM_LAYERS)\n",
        "        ]\n",
        "\n",
        "        # CLS token.\n",
        "        initial_value = tf.zeros((1, 1, PROJECTION_DIM))\n",
        "        self.cls_token = tf.Variable(\n",
        "            initial_value=initial_value, trainable=True, name=\"cls\"\n",
        "        )\n",
        "\n",
        "        # Other layers.\n",
        "        self.dropout = layers.Dropout(DROPOUT_RATE)\n",
        "        self.layer_norm = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)\n",
        "        self.head = layers.Dense(\n",
        "            NUM_CLASSES,\n",
        "            name=\"classification_head\",\n",
        "        )\n",
        "\n",
        "    def call(self, inputs, training=True):\n",
        "        n = tf.shape(inputs)[0]\n",
        "\n",
        "        # 패치를 만들고 projection 시킨다\n",
        "        projected_patches = self.projection(inputs)\n",
        "\n",
        "        # 필요하다면 projected_patches 텐서 뒤에 클래스 토큰을 붙인다\n",
        "        cls_token = tf.tile(self.cls_token, (n, 1, 1))\n",
        "        cls_token = tf.cast(cls_token, projected_patches.dtype)\n",
        "        projected_patches = tf.concat([cls_token, projected_patches], axis=1)\n",
        "\n",
        "        # positional embeddings과 projected patches 합치기\n",
        "        encoded_patches = (\n",
        "            self.positional_embedding + projected_patches\n",
        "        )  # (B, number_patches, projection_dim)\n",
        "        encoded_patches = self.dropout(encoded_patches)\n",
        "\n",
        "        # Transformer 블록을 쌓습니다.\n",
        "        for transformer_module in self.transformer_blocks:\n",
        "            # Add a Transformer block.\n",
        "            encoded_patches = transformer_module(encoded_patches)\n",
        "\n",
        "        # Final layer normalization.\n",
        "        representation = self.layer_norm(encoded_patches)\n",
        "\n",
        "        # Pool representation.\n",
        "        encoded_patches = representation[:, 0]\n",
        "\n",
        "        # Classification head.\n",
        "        output = self.head(encoded_patches)\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJOD9ulrwMvl"
      },
      "source": [
        "\n",
        "이 ViTClassifier 클래스는 ViT를 단독으로 사용할 수 있게 만들었으며 end-to-end 학습이 가능합니다. 이제 DeiT로 확장해 보겠습니다. 다음 그림은 DeiT의 개략도(DeiT 논문에서 가져왔습니다)를 보여줍니다:\n",
        "\n",
        "![](https://i.imgur.com/5lmg2Xs.png)\n",
        "\n",
        "\n",
        "클래스 토큰 외에도 DeiT는 distillation을 위한 또 다른 토큰을 가지고 있습니다. distillation 과정에서 클래스 토큰에 해당하는 로짓은 실제 레이블과 비교되고, distillation 토큰에 해당하는 로짓은 Teacher의 예측과 비교됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VJHpMlZwMvl"
      },
      "outputs": [],
      "source": [
        "# Student 모델\n",
        "class ViTDistilled(ViTClassifier):\n",
        "    def __init__(self, regular_training=False, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.num_tokens = 2\n",
        "        self.regular_training = regular_training\n",
        "\n",
        "        # CLS와 distillation 토큰, 그리고 positional embedding.\n",
        "        init_value = tf.zeros((1, 1, PROJECTION_DIM))\n",
        "        self.dist_token = tf.Variable(init_value, name=\"dist_token\")\n",
        "        self.positional_embedding = tf.Variable(\n",
        "            tf.zeros(\n",
        "                (\n",
        "                    1,\n",
        "                    NUM_PATCHES + self.num_tokens,\n",
        "                    PROJECTION_DIM,\n",
        "                )\n",
        "            ),\n",
        "            name=\"pos_embedding\",\n",
        "        )\n",
        "\n",
        "        # Head layers.\n",
        "        self.head = layers.Dense(\n",
        "            NUM_CLASSES,\n",
        "            name=\"classification_head\",\n",
        "        )\n",
        "        self.head_dist = layers.Dense(\n",
        "            NUM_CLASSES,\n",
        "            name=\"distillation_head\",\n",
        "        )\n",
        "\n",
        "    def call(self, inputs, training=True):\n",
        "        n = tf.shape(inputs)[0]\n",
        "\n",
        "        # 패치를 만들고 projection 시킨다\n",
        "        projected_patches = self.projection(inputs)\n",
        "\n",
        "        # 텐서 뒤에 토큰을 붙인다\n",
        "        cls_token = tf.tile(self.cls_token, (n, 1, 1))\n",
        "        dist_token = tf.tile(self.dist_token, (n, 1, 1))\n",
        "        cls_token = tf.cast(cls_token, projected_patches.dtype)\n",
        "        dist_token = tf.cast(dist_token, projected_patches.dtype)\n",
        "        projected_patches = tf.concat(\n",
        "            [cls_token, dist_token, projected_patches], axis=1\n",
        "        )\n",
        "\n",
        "        # positional embeddings과 projected patches 합치기\n",
        "        encoded_patches = (\n",
        "            self.positional_embedding + projected_patches\n",
        "        )  # (B, number_patches, projection_dim)\n",
        "        encoded_patches = self.dropout(encoded_patches)\n",
        "\n",
        "        # Transformer 블록을 쌓습니다.\n",
        "        for transformer_module in self.transformer_blocks:\n",
        "            # Add a Transformer block.\n",
        "            encoded_patches = transformer_module(encoded_patches)\n",
        "\n",
        "        # Final layer normalization.\n",
        "        representation = self.layer_norm(encoded_patches)\n",
        "\n",
        "        # Classification heads.\n",
        "        x, x_dist = (\n",
        "            self.head(representation[:, 0]),\n",
        "            self.head_dist(representation[:, 1]),\n",
        "        )\n",
        "\n",
        "        if not training or self.regular_training:\n",
        "            # During standard train / finetune, inference average the classifier\n",
        "            # predictions.\n",
        "            return (x + x_dist) / 2\n",
        "\n",
        "        elif training:\n",
        "            # Only return separate classification predictions when training in distilled\n",
        "            # mode.\n",
        "            return x, x_dist\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gU2eezjwMvl"
      },
      "source": [
        "\n",
        "`ViTDistilled` 클래스가 우리의 예상대로 초기화 및 호출될 수 있는지 확인합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ez9kzVPEwMvl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83db0e51-0301-42be-dbbd-314dd77e6b70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 5)\n"
          ]
        }
      ],
      "source": [
        "deit_tiny_distilled = ViTDistilled()\n",
        "\n",
        "dummy_inputs = tf.ones((2, 224, 224, 3))\n",
        "outputs = deit_tiny_distilled(dummy_inputs, training=False)\n",
        "print(outputs.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpgfNi20wMvl"
      },
      "source": [
        "## Trainer 구현하기\n",
        "\n",
        "knowledge distillation\n",
        "([Hinton et al.](https://arxiv.org/abs/1503.02531))에서 일어나는 것과 달리,\n",
        "KL divergence 뿐만 아니라 temperature-scaled softmax가 사용되는 경우, DeiT 저자들은 다음과 같은 손실 함수를 사용했습니다.\n",
        "\n",
        "![](https://i.imgur.com/bXdxsBq.png)\n",
        "\n",
        "\n",
        "여기를 보세요,\n",
        "\n",
        "* CE is cross-entropy\n",
        "* `psi` is the softmax function\n",
        "* Z_s denotes student predictions\n",
        "* y denotes true labels\n",
        "* y_t denotes teacher predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gL4aEjv4wMvm"
      },
      "outputs": [],
      "source": [
        "\n",
        "# knowledge distillation 하는 class\n",
        "class DeiT(keras.Model):\n",
        "    # Reference:\n",
        "    # https://keras.io/examples/vision/knowledge_distillation/\n",
        "    def __init__(self, student, teacher, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.student = student\n",
        "        self.teacher = teacher\n",
        "\n",
        "        self.student_loss_tracker = keras.metrics.Mean(name=\"student_loss\")\n",
        "        self.dist_loss_tracker = keras.metrics.Mean(name=\"distillation_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        metrics = super().metrics\n",
        "        metrics.append(self.student_loss_tracker)\n",
        "        metrics.append(self.dist_loss_tracker)\n",
        "        return metrics\n",
        "\n",
        "    def compile(\n",
        "        self,\n",
        "        optimizer,\n",
        "        metrics,\n",
        "        student_loss_fn,\n",
        "        distillation_loss_fn,\n",
        "    ):\n",
        "        super().compile(optimizer=optimizer, metrics=metrics)\n",
        "        self.student_loss_fn = student_loss_fn\n",
        "        self.distillation_loss_fn = distillation_loss_fn\n",
        "\n",
        "    def train_step(self, data):\n",
        "        # 데이터 풀기\n",
        "        x, y = data\n",
        "\n",
        "        # Teacher의 Forward pass\n",
        "        teacher_predictions = tf.nn.softmax(self.teacher(x, training=False), -1)\n",
        "        teacher_predictions = tf.argmax(teacher_predictions, -1)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Studentdml Forward pass\n",
        "            cls_predictions, dist_predictions = self.student(x / 255.0, training=True)\n",
        "\n",
        "            # student_loss와 distillation_loss 계산\n",
        "            student_loss = self.student_loss_fn(y, cls_predictions)\n",
        "            distillation_loss = self.distillation_loss_fn(\n",
        "                teacher_predictions, dist_predictions\n",
        "            )\n",
        "            loss = (student_loss + distillation_loss) / 2\n",
        "\n",
        "        # Gradients 계산\n",
        "        trainable_vars = self.student.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "\n",
        "        # Weights 업데이트\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "\n",
        "        # `compile()`에 명시된 metrics 업데이트\n",
        "        student_predictions = (cls_predictions + dist_predictions) / 2\n",
        "        self.compiled_metrics.update_state(y, student_predictions)\n",
        "        self.dist_loss_tracker.update_state(distillation_loss)\n",
        "        self.student_loss_tracker.update_state(student_loss)\n",
        "\n",
        "        # 결과값 반환\n",
        "        results = {m.name: m.result() for m in self.metrics}\n",
        "        return results\n",
        "\n",
        "    def test_step(self, data):\n",
        "        # 데이터 풀기\n",
        "        x, y = data\n",
        "\n",
        "        # x 값 대한 결과값 예측\n",
        "        y_prediction = self.student(x / 255.0, training=False)\n",
        "\n",
        "        # loss 계산\n",
        "        student_loss = self.student_loss_fn(y, y_prediction)\n",
        "\n",
        "        # metrics 업데이트\n",
        "        self.compiled_metrics.update_state(y, y_prediction)\n",
        "        self.student_loss_tracker.update_state(student_loss)\n",
        "\n",
        "        # Return a dict of performance.\n",
        "        results = {m.name: m.result() for m in self.metrics}\n",
        "        return results\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.student(inputs / 255.0, training=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ck0VJdRAwMvm"
      },
      "source": [
        "## Teacher 모델 로드하기\n",
        "\n",
        "이 모델은 BiT family of ResNets\n",
        "([Kolesnikov et al.](https://arxiv.org/abs/1912.11370))에 기반했고 `tf_flowers`으로 fine-tuned 했습니다. 이 [this notebook](https://github.com/sayakpaul/deit-tf/blob/main/notebooks/bit-teacher.ipynb)을 참조하여 학습이 어떻게 되었는지 알 수 있습니다. Teacher 모델은 Student보다 **약 40배** 많은 약 2억 2천만 개의 파라미터를 가지고 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wC3aNDBwMvm"
      },
      "outputs": [],
      "source": [
        "!wget -q https://github.com/sayakpaul/deit-tf/releases/download/v0.1.0/bit_teacher_flowers.zip\n",
        "!unzip -q bit_teacher_flowers.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAdaozhfwMvm"
      },
      "outputs": [],
      "source": [
        "bit_teacher_flowers = keras.models.load_model(\"bit_teacher_flowers\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WP7Y31dmwMvm"
      },
      "source": [
        "## Distillation으로 Student 모델 학습하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDdF4K02wMvm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9fce957-c2fc-4198-99d2-a6d88b9651d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        }
      ],
      "source": [
        "deit_tiny = ViTDistilled()\n",
        "deit_distiller = DeiT(student=deit_tiny, teacher=bit_teacher_flowers)\n",
        "\n",
        "lr_scaled = (BASE_LR / 512) * BATCH_SIZE\n",
        "deit_distiller.compile(\n",
        "    optimizer=tfa.optimizers.AdamW(weight_decay=WEIGHT_DECAY, learning_rate=lr_scaled),\n",
        "    metrics=[\"accuracy\"],\n",
        "    student_loss_fn=keras.losses.CategoricalCrossentropy(\n",
        "        from_logits=True, label_smoothing=0.1\n",
        "    ),\n",
        "    distillation_loss_fn=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        ")\n",
        "_ = deit_distiller.fit(train_dataset, validation_data=val_dataset, epochs=NUM_EPOCHS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXmO1H1pwMvm"
      },
      "source": [
        "만약 우리가 똑같은 하이퍼파라미터를 가지고 스크래치로 같은 모델( `ViTClassifier`)을 훈련했다면, 그 모델은 약 59%의 정확도를 얻었을 것입니다. 그 결과를 재현하기 위해 아래와 같이 코드를 바꿔보면 됩니다 :\n",
        "```\n",
        "vit_tiny = ViTClassifier()\n",
        "\n",
        "inputs = keras.Input((RESOLUTION, RESOLUTION, 3))\n",
        "x = keras.layers.Rescaling(scale=1./255)(inputs)\n",
        "outputs = deit_tiny(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "model.compile(...)\n",
        "model.fit(...)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LugsBC7wMvm"
      },
      "source": [
        "## 노트\n",
        "\n",
        "* Distillation 사용을 통해 CNN 기반 Teacher 모델의 inductive biases을 효과적으로 전달하고 있습니다.\n",
        "* 흥미롭게도, 이 distillation 전략은 논문에서 보여지는 teacher CNN을 사용했을 때 모델로 트랜스포머보다 더 잘 작동합니다.\n",
        "* DeiT 모델을 훈련시키기 위해 정규화(regularization)를 사용하는 것은 매우 중요합니다.\n",
        "* ViT 모델은 truncated normal, random normal, Glorot uniform 등을 포함한 다양한 이니셜라이저의 조합으로 초기화됩니다. 원래 결과의 end-to-end 복제를 원하는 경우 ViT를 주의해서 초기화해야 합니다.\n",
        "* Fine-tuning을 위해 TensorFlow와 Keras로 pre-trained된 DeiT 모델을 찾으려면 [TF-Hub](https://tfhub.dev/sayakpaul/collections/deit/1)을 사용하면 됩니다.\n",
        "\n",
        "## 감사의 말\n",
        "\n",
        "* Ross Wightman은 [`timm`](https://github.com/rwightman/pytorch-image-models)을 최신 상태로 유지해줍니다. TensorFlow로 구현하면서 그의 ViT와 DeiT의 구현을 많이 참고했습니다.\n",
        "* 다른 프로젝트에서 `ViTClassifier의 일부를 [Aritra Roy Gosthipaty](https://github.com/ariG23498)가 구현했습니다.\n",
        "\n",
        "* [Google Developers Experts](https://developers.google.com/programs/experts/) 프로그램은 이 예에 대한 실험을 실행하는 데 사용된 GCP 크레딧으로 저를 지원해주는 프로그램입니다.\n",
        "\n",
        "Example available on HuggingFace:\n",
        "\n",
        "| Trained Model | Demo |\n",
        "| :--: | :--: |\n",
        "| [![Generic badge](https://img.shields.io/badge/🤗%20Model-DEIT-black.svg)](https://huggingface.co/keras-io/deit) | [![Generic badge](https://img.shields.io/badge/🤗%20Spaces-DEIT-black.svg)](https://huggingface.co/spaces/keras-io/deit/) |"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GqaqQmMB7fm6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
